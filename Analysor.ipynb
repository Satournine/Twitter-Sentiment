{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da973d9-60e0-421b-8f48-21eafba5e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from TwToken import consumer_key, consumer_secret, access_token, access_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6444df-29dc-4b68-9301-09f61fdcebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7997f56-0580-4519-8fe6-a255223ca7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "SearchQuery = \"Twitter\" #Keyword to search\n",
    "def get_dataset():\n",
    "    print(\"Mining tweets...\")\n",
    "    mined_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                               q=SearchQuery + \" -filter:retweets\",\n",
    "                               tweet_mode=\"extended\",\n",
    "                               lang=\"en\").items(5000):\n",
    "        mined_tweets.append(tweet.full_text)\n",
    "    print(\"Mined tweets : {}\".format(len(mined_tweets)))\n",
    "    return mined_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41234b-f5d2-498c-b613-35aa55a58692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35193d12-0afa-4122-870c-54b0014ee08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    token_list = []\n",
    "    token_list = word_tokenize(tweet)\n",
    "    token_list = [token for token in token_list if token not in string.punctuation]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.discard('not')\n",
    "    token_list = [token for token in token_list if not token in stop_words]\n",
    "    token_list = [token for token in token_list if token.isalpha()]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb35fd-eb3b-4fc9-99ae-6d3956bad759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        token_list.append(stemmer.stem(token))\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df61691-9fe3-4db6-83be-54a695cf20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweet):\n",
    "    tweet = re.sub(r'[,\\.!?]', \"\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = decontracted(tweet)\n",
    "    tweet = p.clean(tweet)\n",
    "    tokens = tokenize(tweet)\n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    stem = stem_tokens(tokens, snowball_stemmer)\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810876b0-d41c-49f6-9ab7-8d165d39b0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_tfidf(corpus):\n",
    "    tf_vect = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "    tf_vect.fit(corpus)\n",
    "    return tf_vect\n",
    "def fit_lr(X_train, y_train):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "df = pd.read_csv(\"tweet_data.csv\")\n",
    "df[\"tokens\"] = df[\"tweet_text\"].apply(process_tweets)\n",
    "df[\"tweet_sentiment\"] = df[\"sentiment\"].apply(lambda i: 1 if i == \"positive\" else 0)\n",
    "X = df[\"tokens\"].tolist()\n",
    "y = df[\"tweet_sentiment\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80)\n",
    "\n",
    "tf = fit_tfidf(X_train)\n",
    "X_train_tf = tf.transform(X_train)\n",
    "X_test_tf = tf.transform(X_test)\n",
    "\n",
    "model_lr_tf = fit_lr(X_train_tf, y_train)\n",
    "\n",
    "positive_tweets = []\n",
    "counterPos = []\n",
    "counterNeg = []\n",
    "negative_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f1f3a-310d-4ff5-a55c-bf09531d2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames():                  # plot func\n",
    "    tweetDf = pd.read_csv('Tweets.csv')\n",
    "    sns.countplot(data=tweetDf,x = 'Sentiment', palette='hls')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4555552-5524-49bb-9682-cd80b6b0fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet):\n",
    "    processed_tweet = process_tweets(tweet)\n",
    "    transformed_tweet = tf.transform([processed_tweet])\n",
    "    prediction = model_lr_tf.predict(transformed_tweet)\n",
    "\n",
    "    if prediction == 1:\n",
    "        positive_tweets.append(processed_tweet)\n",
    "\n",
    "\n",
    "    else:\n",
    "        negative_tweets.append(processed_tweet)\n",
    "\n",
    "def main():\n",
    "    tweets = get_dataset()\n",
    "    for tweet in tweets:\n",
    "        predict_tweet(tweet)\n",
    "\n",
    "    counterPos = ['Positive'] * len(positive_tweets)\n",
    "    counterNeg = ['Negative'] * len(negative_tweets)\n",
    "\n",
    "    positiveZipped = list(zip(positive_tweets,counterPos))  # tags tweets positive or negative\n",
    "    negativeZipped = list(zip(negative_tweets, counterNeg))\n",
    "\n",
    "    finalList = positiveZipped + negativeZipped  # concatenate\n",
    "\n",
    "    tweetDf = pd.DataFrame(finalList,columns=['Tweets', 'Sentiment']) #new csv\n",
    "    tweetDf.to_csv('Tweets.csv')\n",
    "    print(tweetDf.tail)\n",
    "\n",
    "    print(\"Positive tweets : {}\".format(len(positive_tweets)))\n",
    "\n",
    "    print(\"Negative tweets : {}\".format(len(negative_tweets)))\n",
    "    plot_frames()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
